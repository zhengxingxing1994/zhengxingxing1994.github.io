<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> CS231n Assignment3_Image Captioning with RNNs | Liangliang Zheng </title> <meta name="author" content="Liangliang Zheng"> <meta name="description" content="Welcome to Liangliang's blog, where I share my thoughts and experiences on various topics. "> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://paranoiarchive.com//blog/2018/cs231n-assignment3_image-captioning-with-rnns/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>initTheme();</script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Liangliang</span> Zheng </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item "> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">archive </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/archive_by_year">by year</a> <a class="dropdown-item " href="/archive_by_tag">by category</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">CS231n Assignment3_Image Captioning with RNNs</h1> <p class="post-meta"> Created in August 03, 2018 </p> <p class="post-tags"> <a href="/blog/2018"> <i class="fa-solid fa-calendar fa-sm"></i> 2018 </a>   ·   <a href="/blog/category/dl-ml-python"> <i class="fa-solid fa-tag fa-sm"></i> dl-ml-python</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <p>Yesterday I finished a vanilla recurrent neural networks and used them to train a model that could generate novel captions for images. It’s really excited that as the the domain of NLP, word embedding can be combining with Computer Vision CNN for image captioning,which is sort of like Lego construction.(All the images drawn in draft are from <a href="https://www.cnblogs.com/hellcat/p/7191967.html" rel="external nofollow noopener" target="_blank">link</a> )</p> <p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_112358.jpg" alt="2018-08-03_112358.jpg"></p> <p>The whole process will be pretty much as following:</p> <p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_112636.jpg" alt="2018-08-03_112636"></p> <hr> <p>Vanilla RNN single step forward:</p> <p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_100635.jpg" alt="2018-08-03_100635.jpg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">rnn_step_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">prev_h</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
 <span class="mi">2</span>  <span class="sh">"""</span><span class="s">
 3  Run the forward pass for a single timestep of a vanilla RNN that uses a tanh
 4  activation function.
 5 
 6  The input data has dimension D, the hidden state has dimension H, and we use
 7  a minibatch size of N.
 8 
 9  Inputs:
10   - x: Input data for this timestep, of shape (N, D).
11   - prev_h: Hidden state from previous timestep, of shape (N, H)
12   - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
13   - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
14   - b: Biases of shape (H,)
15  
16   Returns a tuple of:
17   - next_h: Next hidden state, of shape (N, H)
18   - cache: Tuple of values needed for the backward pass.
19   </span><span class="sh">"""</span>
<span class="mi">20</span>   <span class="n">next_h</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
<span class="mi">21</span>   <span class="c1">##############################################################################
</span><span class="mi">22</span>   <span class="c1"># TODO: Implement a single forward step for the vanilla RNN. Store the next  #
</span><span class="mi">23</span>   <span class="c1"># hidden state and any values you need for the backward pass in the next_h   #
</span><span class="mi">24</span>   <span class="c1"># and cache variables respectively.                                          #
</span><span class="mi">25</span>   <span class="c1">##############################################################################
</span><span class="mi">26</span>   <span class="n">next_h</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">tanh</span><span class="p">(</span><span class="n">prev_h</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Wh</span><span class="p">)</span> <span class="o">+</span> <span class="n">x</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">Wx</span><span class="p">)</span> <span class="o">+</span> <span class="n">b</span><span class="p">)</span>
<span class="mi">27</span>   <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span><span class="p">,</span><span class="n">Wx</span><span class="p">,</span><span class="n">Wh</span><span class="p">,</span><span class="n">prev_h</span><span class="p">,</span><span class="n">next_h</span><span class="p">)</span>
<span class="mi">28</span>   <span class="c1">#pass
</span><span class="mi">29</span>     <span class="c1">##############################################################################
</span><span class="mi">30</span>     <span class="c1">#                               END OF YOUR CODE                             #
</span><span class="mi">31</span>     <span class="c1">##############################################################################
</span><span class="mi">32</span>   <span class="k">return</span> <span class="n">next_h</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div> <p>Notice these two parameters: - Wx: Weight matrix for input-to-hidden connections, of shape (D, H) - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)</p> <p>RNN one step backward, according to their size and pay attention to the transpose.<img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_101459.jpg" alt="2018-08-03_101459.jpg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">rnn_step_backward</span><span class="p">(</span><span class="n">dnext_h</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Backward pass for a single timestep of a vanilla RNN.
 4    Inputs:
 5    - dnext_h: Gradient of loss with respect to next hidden state, of shape (N, H)
 6    - cache: Cache object from the forward pass
 7    Returns a tuple of:
 8    - dx: Gradients of input data, of shape (N, D)
 9    - dprev_h: Gradients of previous hidden state, of shape (N, H)
10     - dWx: Gradients of input-to-hidden weights, of shape (D, H)
11     - dWh: Gradients of hidden-to-hidden weights, of shape (H, H)
12     - db: Gradients of bias vector, of shape (H,)
13     </span><span class="sh">"""</span>
<span class="mi">14</span>     <span class="n">dx</span><span class="p">,</span> <span class="n">dprev_h</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
<span class="mi">15</span>     <span class="c1">##############################################################################
</span><span class="mi">16</span>     <span class="c1"># TODO: Implement the backward pass for a single step of a vanilla RNN.      #
</span><span class="mi">17</span>     <span class="c1">#                                                                            #
</span><span class="mi">18</span>     <span class="c1"># HINT: For the tanh function, you can compute the local derivative in terms #
</span><span class="mi">19</span>     <span class="c1"># of the output value from tanh.                                             #
</span><span class="mi">20</span>     <span class="c1">##############################################################################
</span><span class="mi">21</span>     <span class="n">x</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">prev_h</span><span class="p">,</span> <span class="n">next_h</span> <span class="o">=</span> <span class="n">cache</span>
<span class="mi">22</span>     <span class="n">d_tanh</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">next_h</span><span class="o">**</span><span class="mi">2</span>
<span class="mi">23</span>     <span class="n">dx</span> <span class="o">=</span> <span class="p">(</span><span class="n">dnext_h</span><span class="o">*</span><span class="n">dtanh</span><span class="p">).</span><span class="nf">dot</span><span class="p">(</span><span class="n">Wx</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="mi">24</span>     <span class="n">dWx</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">dnext_h</span><span class="o">*</span><span class="n">dtanh</span><span class="p">)</span>
<span class="mi">25</span>     <span class="n">dprev_h</span> <span class="o">=</span> <span class="p">(</span><span class="n">dnext_h</span><span class="o">*</span><span class="n">dtanh</span><span class="p">).</span><span class="nf">dot</span><span class="p">(</span><span class="n">Wh</span><span class="p">.</span><span class="n">T</span><span class="p">)</span>
<span class="mi">26</span>     <span class="n">dWh</span> <span class="o">=</span> <span class="n">prev_h</span><span class="p">.</span><span class="n">T</span><span class="p">.</span><span class="nf">dot</span><span class="p">(</span><span class="n">dnext_h</span><span class="o">*</span><span class="n">dtanh</span><span class="p">)</span>
<span class="mi">27</span>     <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dnext_h</span><span class="o">*</span><span class="n">dtanh</span><span class="p">,</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="c1"># 按列相加
</span><span class="mi">28</span>     <span class="c1">#pass
</span><span class="mi">29</span>     <span class="c1">##############################################################################
</span><span class="mi">30</span>     <span class="c1">#                               END OF YOUR CODE                             #
</span><span class="mi">31</span>     <span class="c1">##############################################################################
</span><span class="mi">32</span>     <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dprev_h</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div> <p>After single step, we need to finish the reccurent loop part</p> <p><img src="https://zhengliangliang.files.wordpress.com/2018/08/1161096-20170716200928128-376197048.jpg" alt=""><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_102633.jpg" alt="2018-08-03_102633.jpg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">rnn_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">h0</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Run a vanilla RNN forward on an entire sequence of data. We assume an input
 4    sequence composed of T vectors, each of dimension D. The RNN uses a hidden
 5    size of H, and we work over a minibatch containing N sequences. After running
 6    the RNN forward, we return the hidden states for all timesteps.
 7    Inputs:
 8    - x: Input data for the entire timeseries, of shape (N, T, D).
 9    - h0: Initial hidden state, of shape (N, H)
10     - Wx: Weight matrix for input-to-hidden connections, of shape (D, H)
11     - Wh: Weight matrix for hidden-to-hidden connections, of shape (H, H)
12     - b: Biases of shape (H,)
13     Returns a tuple of:
14     - h: Hidden states for the entire timeseries, of shape (N, T, H).
15     - cache: Values needed in the backward pass
16     </span><span class="sh">"""</span>
<span class="mi">17</span>     <span class="n">h</span><span class="p">,</span> <span class="n">cache</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
<span class="mi">18</span>     <span class="c1">##############################################################################
</span><span class="mi">19</span>     <span class="c1"># TODO: Implement forward pass for a vanilla RNN running on a sequence of    #
</span><span class="mi">20</span>     <span class="c1"># input data. You should use the rnn_step_forward function that you defined  #
</span><span class="mi">21</span>     <span class="c1"># above. You can use a for loop to help compute the forward pass.            #
</span><span class="mi">22</span>     <span class="c1">##############################################################################
</span><span class="mi">23</span>     <span class="c1"># get the size of x 
</span><span class="mi">24</span> 	<span class="n">N</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
<span class="mi">25</span> 	<span class="n">_</span><span class="p">,</span><span class="n">H</span> <span class="o">=</span> <span class="n">h0</span><span class="p">.</span><span class="n">shape</span>
<span class="mi">26</span> 	<span class="c1"># initialize the hidden h
</span><span class="mi">27</span> 	<span class="n">h</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeors</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">H</span><span class="p">))</span>
<span class="mi">28</span> 	<span class="n">cache</span> <span class="o">=</span> <span class="p">[]</span>
<span class="mi">29</span> 	<span class="n">h_next</span> <span class="o">=</span> <span class="n">h0</span>
<span class="mi">30</span> 	<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="p">):</span>
<span class="mi">31</span> 		<span class="n">h</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:],</span><span class="n">cache_next</span> <span class="o">=</span> <span class="nf">rnn_step_forward</span><span class="p">(</span><span class="n">x</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:],</span> <span class="n">h_next</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="mi">32</span> 		<span class="n">h_next</span> <span class="o">=</span> <span class="n">h</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:]</span>
<span class="mi">33</span> 		<span class="n">cache</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">cache_next</span><span class="p">)</span>
<span class="mi">34</span>     <span class="c1">#pass
</span><span class="mi">35</span>     <span class="c1">##############################################################################
</span><span class="mi">36</span>     <span class="c1">#                               END OF YOUR CODE                             #
</span><span class="mi">37</span>     <span class="c1">##############################################################################
</span><span class="mi">38</span>     <span class="k">return</span> <span class="n">h</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div> <p>For the Backward RNN: <img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_104145.jpg" alt="2018-08-03_104145.jpg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">rnn_backward</span><span class="p">(</span><span class="n">dh</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Compute the backward pass for a vanilla RNN over an entire sequence of data.
 4    Inputs:
 5- dh: Upstream gradients of all hidden states, of shape (N, T, H). 
 6    
 7NOTE: </span><span class="sh">'</span><span class="s">dh</span><span class="sh">'</span><span class="s"> contains the upstream gradients produced by the 
 8    individual loss functions at each timestep, *not* the gradients
 9    being passed between timesteps (which you</span><span class="sh">'</span><span class="s">ll have to compute yourself
10     by calling rnn_step_backward in a loop).
11     Returns a tuple of:
12     - dx: Gradient of inputs, of shape (N, T, D)
13     - dh0: Gradient of initial hidden state, of shape (N, H)
14     - dWx: Gradient of input-to-hidden weights, of shape (D, H)
15     - dWh: Gradient of hidden-to-hidden weights, of shape (H, H)
16     - db: Gradient of biases, of shape (H,)
17     </span><span class="sh">"""</span>
<span class="mi">18</span>     <span class="n">dx</span><span class="p">,</span> <span class="n">dh0</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span> <span class="o">=</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span><span class="p">,</span> <span class="bp">None</span>
<span class="mi">19</span>     <span class="c1">##############################################################################
</span><span class="mi">20</span>     <span class="c1"># TODO: Implement the backward pass for a vanilla RNN running an entire      #
</span><span class="mi">21</span>     <span class="c1"># sequence of data. You should use the rnn_step_backward function that you   #
</span><span class="mi">22</span>     <span class="c1"># defined above. You can use a for loop to help compute the backward pass.   #
</span><span class="mi">23</span>     <span class="c1">##############################################################################
</span><span class="mi">24</span>     <span class="n">x</span><span class="p">,</span><span class="n">Wx</span><span class="p">,</span><span class="n">Wh</span><span class="p">,</span><span class="n">prev_h</span><span class="p">,</span><span class="n">nexth</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># start from the final one 
</span><span class="mi">25</span>     <span class="n">x</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">prev_h</span><span class="p">,</span> <span class="n">next_h</span> <span class="o">=</span> <span class="n">cache</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span> <span class="c1"># start from the final one
</span><span class="mi">26</span>     <span class="n">_</span><span class="p">,</span><span class="n">D</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="n">shape</span>
<span class="mi">27</span>     <span class="n">N</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">H</span> <span class="o">=</span> <span class="n">dh</span><span class="p">.</span><span class="n">shape</span>
<span class="mi">28</span>     <span class="n">dx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">T</span><span class="p">,</span><span class="n">D</span><span class="p">))</span> <span class="c1"># initialization
</span><span class="mi">29</span>     <span class="n">dh0</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">H</span><span class="p">))</span>
<span class="mi">30</span>     <span class="n">dWx</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">D</span><span class="p">,</span><span class="n">H</span><span class="p">))</span>
<span class="mi">31</span>     <span class="n">dWh</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">H</span><span class="p">,</span><span class="n">H</span><span class="p">))</span>
<span class="mi">32</span>     <span class="n">db</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">H</span><span class="p">)</span>
<span class="mi">33</span>     <span class="n">dprev_h_</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros</span><span class="p">((</span><span class="n">N</span><span class="p">,</span><span class="n">H</span><span class="p">))</span>
<span class="mi">34</span>     <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">T</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">):</span> <span class="c1"># start from the final one
</span><span class="mi">35</span>         <span class="n">dx_</span><span class="p">,</span> <span class="n">dprev_h_</span><span class="p">,</span> <span class="n">dWx_</span><span class="p">,</span> <span class="n">dWh_</span><span class="p">,</span> <span class="n">db_</span> <span class="o">=</span> <span class="nf">rnn_step_backward</span><span class="p">(</span><span class="n">dh</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:]</span> <span class="o">+</span> <span class="n">dprev_h_</span><span class="p">,</span><span class="n">cache</span><span class="p">.</span><span class="nf">pop</span><span class="p">())</span>
<span class="mi">36</span>         <span class="n">dx</span><span class="p">[:,</span><span class="n">i</span><span class="p">,:]</span> <span class="o">=</span> <span class="n">dx_</span>
<span class="mi">37</span>         <span class="n">dh0</span> <span class="o">=</span> <span class="n">dprev_h_</span>
<span class="mi">38</span>         <span class="n">dWx</span> <span class="o">+=</span> <span class="n">dWx_</span>
<span class="mi">39</span>         <span class="n">dWh</span> <span class="o">+=</span> <span class="n">dWh_</span>
<span class="mi">40</span>         <span class="n">db</span> <span class="o">+=</span> <span class="n">db_</span>	
<span class="mi">41</span>     <span class="c1">#pass
</span><span class="mi">42</span>     <span class="c1">##############################################################################
</span><span class="mi">43</span>     <span class="c1">#                               END OF YOUR CODE                             #
</span><span class="mi">44</span>     <span class="c1">##############################################################################
</span><span class="mi">45</span>     <span class="k">return</span> <span class="n">dx</span><span class="p">,</span> <span class="n">dh0</span><span class="p">,</span> <span class="n">dWx</span><span class="p">,</span> <span class="n">dWh</span><span class="p">,</span> <span class="n">db</span>
</code></pre></div></div> <p>Word_embedding : FORWARD , In deep learning systems, we commonly represent words using vectors. Each word of the vocabulary will be associated with a vector, and these vectors will be learned jointly with the rest of the system. The whole process will be like this one , from caption_in to the X is the word_embedding process: <img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_110902.jpg" alt="2018-08-03_110902.jpg"></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">word_embedding_forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">W</span><span class="p">):</span>
 <span class="mi">2</span>  <span class="sh">"""</span><span class="s">
 3  Forward pass for word embeddings. We operate on minibatches of size N where
 4  each sequence has length T. We assume a vocabulary of V words, assigning each
 5  to a vector of dimension D.
 6   
 7  Inputs:
 8  - x: Integer array of shape (N, T) giving indices of words. Each element idx
 9    of x muxt be in the range 0 &lt;= idx &lt; V.
10   - W: Weight matrix of shape (V, D) giving word vectors for all words.
11    
12   Returns a tuple of:
13   - out: Array of shape (N, T, D) giving word vectors for all input words.
14   - cache: Values needed for the backward pass
15   </span><span class="sh">"""</span>
<span class="mi">16</span>  
<span class="mi">17</span>   <span class="n">out</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">x</span><span class="p">,</span> <span class="p">:]</span>
<span class="mi">18</span>   <span class="n">cache</span> <span class="o">=</span> <span class="p">(</span><span class="n">W</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
<span class="mi">19</span>    
<span class="mi">20</span>   <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">cache</span>
</code></pre></div></div> <p>This process just choose the giving indices of words from the vectors of all words And the backward will be like : </p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span> <span class="k">def</span> <span class="nf">word_embedding_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">cache</span><span class="p">):</span>
 <span class="mi">2</span>    <span class="sh">"""</span><span class="s">
 3    Backward pass for word embeddings. We cannot back-propagate into the words
 4    since they are integers, so we only return gradient for the word embedding
 5    matrix.
 6    HINT: Look up the function np.add.at
 7    Inputs:
 8    - dout: Upstream gradients of shape (N, T, D)
 9    - cache: Values from the forward pass
10     Returns:
11     - dW: Gradient of word embedding matrix, of shape (V, D).
12     </span><span class="sh">"""</span>
<span class="mi">13</span>     <span class="n">dW</span> <span class="o">=</span> <span class="bp">None</span>
<span class="mi">14</span>     <span class="c1">##############################################################################
</span><span class="mi">15</span>     <span class="c1"># TODO: Implement the backward pass for word embeddings.                     #
</span><span class="mi">16</span>     <span class="c1">#                                                                            #
</span><span class="mi">17</span>     <span class="c1"># Note that words can appear more than once in a sequence.                   #
</span><span class="mi">18</span>     <span class="c1"># HINT: Look up the function np.add.at                                       #
</span><span class="mi">19</span>     <span class="c1">##############################################################################
</span><span class="mi">20</span>     <span class="n">W</span><span class="p">,</span> <span class="n">x</span> <span class="o">=</span> <span class="n">cache</span>
<span class="mi">21</span>     <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">W</span><span class="p">)</span>
<span class="mi">22</span> 	<span class="c1"># add dout at the indices x TO dW
</span><span class="mi">23</span>     <span class="n">np</span><span class="p">.</span><span class="n">add</span><span class="p">.</span><span class="nf">at</span><span class="p">(</span><span class="n">dW</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">dout</span><span class="p">)</span>
<span class="mi">24</span>     <span class="c1">#pass
</span><span class="mi">25</span>     <span class="c1">##############################################################################
</span><span class="mi">26</span>     <span class="c1">#                               END OF YOUR CODE                             #
</span><span class="mi">27</span>     <span class="c1">##############################################################################
</span><span class="mi">28</span>     <span class="k">return</span> <span class="n">dW</span>
</code></pre></div></div> <p>Notice that in evert timestop we should use an afine function to transform the RNN hidden vector at vector into scores for each word，we omit this because I have implemented it in assignment2, if you want to see the code , you can go to my github,and in this <a href="https://github.com/ZhengLiangliang1996/CS231n/blob/master/assignment3/cs231n/rnn_layers.py#L209" rel="external nofollow noopener" target="_blank">file</a>. in function temporal_affine_forward/backward</p> <p>At every timestep we produce a score for each word in vocabulary, then use the ground truth word to compute the softmax loss function:<a href="https://github.com/ZhengLiangliang1996/CS231n/blob/master/assignment3/cs231n/rnn_layers.py#L209" rel="external nofollow noopener" target="_blank">file</a></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="mi">1</span>    <span class="k">def</span> <span class="nf">loss</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">features</span><span class="p">,</span> <span class="n">captions</span><span class="p">):</span>
 <span class="mi">2</span>        <span class="sh">"""</span><span class="s">
 3        Compute training-time loss for the RNN. We input image features and
 4        ground-truth captions for those images, and use an RNN (or LSTM) to compute
 5        loss and gradients on all parameters.
 6        Inputs:
 7        - features: Input image features, of shape (N, D)
 8        - captions: Ground-truth captions; an integer array of shape (N, T) where
 9          each element is in the range 0 &lt;= y[i, t] &lt; V
10         Returns a tuple of:
11         - loss: Scalar loss
12         - grads: Dictionary of gradients parallel to self.params
13         </span><span class="sh">"""</span>
<span class="mi">14</span>         <span class="c1"># Cut captions into two pieces: captions_in has everything but the last word
</span><span class="mi">15</span>         <span class="c1"># and will be input to the RNN; captions_out has everything but the first
</span><span class="mi">16</span>         <span class="c1"># word and this is what we will expect the RNN to generate. These are offset
</span><span class="mi">17</span>         <span class="c1"># by one relative to each other because the RNN should produce word (t+1)
</span><span class="mi">18</span>         <span class="c1"># after receiving word t. The first element of captions_in will be the START
</span><span class="mi">19</span>         <span class="c1"># token, and the first element of captions_out will be the first word.
</span><span class="mi">20</span>         <span class="n">captions_in</span> <span class="o">=</span> <span class="n">captions</span><span class="p">[:,</span> <span class="p">:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
<span class="mi">21</span>         <span class="n">captions_out</span> <span class="o">=</span> <span class="n">captions</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">:]</span>
<span class="mi">22</span> 
<span class="mi">23</span>         <span class="c1"># You'll need this
</span><span class="mi">24</span>         <span class="n">mask</span> <span class="o">=</span> <span class="p">(</span><span class="n">captions_out</span> <span class="o">!=</span> <span class="n">self</span><span class="p">.</span><span class="n">_null</span><span class="p">)</span>
<span class="mi">25</span> 
<span class="mi">26</span>         <span class="c1"># Weight and bias for the affine transform from image features to initial
</span><span class="mi">27</span>         <span class="c1"># hidden state
</span><span class="mi">28</span>         <span class="n">W_proj</span><span class="p">,</span> <span class="n">b_proj</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W_proj</span><span class="sh">'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b_proj</span><span class="sh">'</span><span class="p">]</span>
<span class="mi">29</span> 
<span class="mi">30</span>         <span class="c1"># Word embedding matrix
</span><span class="mi">31</span>         <span class="n">W_embed</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W_embed</span><span class="sh">'</span><span class="p">]</span>
<span class="mi">32</span> 
<span class="mi">33</span>         <span class="c1"># Input-to-hidden, hidden-to-hidden, and biases for the RNN
</span><span class="mi">34</span>         <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">Wx</span><span class="sh">'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">Wh</span><span class="sh">'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">]</span>
<span class="mi">35</span> 
<span class="mi">36</span>         <span class="c1"># Weight and bias for the hidden-to-vocab transformation.
</span><span class="mi">37</span>         <span class="n">W_vocab</span><span class="p">,</span> <span class="n">b_vocab</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">W_vocab</span><span class="sh">'</span><span class="p">],</span> <span class="n">self</span><span class="p">.</span><span class="n">params</span><span class="p">[</span><span class="sh">'</span><span class="s">b_vocab</span><span class="sh">'</span><span class="p">]</span>
<span class="mi">38</span> 
<span class="mi">39</span>         <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span> <span class="p">{}</span>
<span class="mi">40</span>         <span class="c1">############################################################################
</span><span class="mi">41</span>         <span class="c1"># TODO: Implement the forward and backward passes for the CaptioningRNN.   #
</span><span class="mi">42</span>         <span class="c1"># In the forward pass you will need to do the following:                   #
</span><span class="mi">43</span>         <span class="c1"># (1) Use an affine transformation to compute the initial hidden state     #
</span><span class="mi">44</span>         <span class="c1">#     from the image features. This should produce an array of shape (N, H)#
</span><span class="mi">45</span>         <span class="c1"># (2) Use a word embedding layer to transform the words in captions_in     #
</span><span class="mi">46</span>         <span class="c1">#     from indices to vectors, giving an array of shape (N, T, W).         #
</span><span class="mi">47</span>         <span class="c1"># (3) Use either a vanilla RNN or LSTM (depending on self.cell_type) to    #
</span><span class="mi">48</span>         <span class="c1">#     process the sequence of input word vectors and produce hidden state  #
</span><span class="mi">49</span>         <span class="c1">#     vectors for all timesteps, producing an array of shape (N, T, H).    #
</span><span class="mi">50</span>         <span class="c1"># (4) Use a (temporal) affine transformation to compute scores over the    #
</span><span class="mi">51</span>         <span class="c1">#     vocabulary at every timestep using the hidden states, giving an      #
</span><span class="mi">52</span>         <span class="c1">#     array of shape (N, T, V).                                            #
</span><span class="mi">53</span>         <span class="c1"># (5) Use (temporal) softmax to compute loss using captions_out, ignoring  #
</span><span class="mi">54</span>         <span class="c1">#     the points where the output word is &lt;NULL&gt; using the mask above.     #
</span><span class="mi">55</span>         <span class="c1">#                                                                          #
</span><span class="mi">56</span>         <span class="c1"># In the backward pass you will need to compute the gradient of the loss   #
</span><span class="mi">57</span>         <span class="c1"># with respect to all model parameters. Use the loss and grads variables   #
</span><span class="mi">58</span>         <span class="c1"># defined above to store loss and gradients; grads[k] should give the      #
</span><span class="mi">59</span>         <span class="c1"># gradients for self.params[k].                                            #
</span><span class="mi">60</span>         <span class="c1">#                                                                          #
</span><span class="mi">61</span>         <span class="c1"># Note also that you are allowed to make use of functions from layers.py   #
</span><span class="mi">62</span>         <span class="c1"># in your implementation, if needed.                                       #
</span><span class="mi">63</span>         <span class="c1">############################################################################
</span><span class="mi">64</span>         <span class="c1"># Word Embedding
</span><span class="mi">65</span>         <span class="n">captions_in_emb</span><span class="p">,</span><span class="n">emb_cache</span> <span class="o">=</span> <span class="nf">word_embedding_forward</span><span class="p">(</span><span class="n">captions_in</span><span class="p">,</span><span class="n">W_embed</span><span class="p">)</span>
<span class="mi">66</span>         <span class="c1"># Affine Forward 
</span><span class="mi">67</span>         <span class="n">h_0</span><span class="p">,</span><span class="n">feature_cache</span> <span class="o">=</span> <span class="nf">affine_forward</span><span class="p">(</span><span class="n">features</span><span class="p">,</span><span class="n">W_proj</span><span class="p">,</span><span class="n">b_proj</span><span class="p">)</span>
<span class="mi">68</span>         <span class="c1">#RNN part
</span><span class="mi">69</span>         <span class="n">h</span><span class="p">,</span><span class="n">rnn_cache</span> <span class="o">=</span> <span class="nf">rnn_forward</span><span class="p">(</span><span class="n">captions_in_emb</span><span class="p">,</span> <span class="n">h_0</span><span class="p">,</span> <span class="n">Wx</span><span class="p">,</span> <span class="n">Wh</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span>
<span class="mi">70</span>         
<span class="mi">71</span>         <span class="c1"># Temporal Afine 
</span><span class="mi">72</span>         <span class="n">temporal_out</span><span class="p">,</span> <span class="n">temporal_cache</span> <span class="o">=</span> <span class="nf">temporal_affine_forward</span><span class="p">(</span><span class="n">h</span><span class="p">,</span> <span class="n">W_vocab</span><span class="p">,</span> <span class="n">b_vocab</span><span class="p">)</span>
<span class="mi">73</span>         
<span class="mi">74</span>         <span class="c1"># Softloss 
</span><span class="mi">75</span>         <span class="n">loss</span><span class="p">,</span> <span class="n">dout</span> <span class="o">=</span> <span class="nf">temporal_softmax_loss</span><span class="p">(</span><span class="n">temporal_out</span><span class="p">,</span> <span class="n">captions_out</span><span class="p">,</span> <span class="n">mask</span><span class="p">)</span>
<span class="mi">76</span>         
<span class="mi">77</span>         <span class="c1"># Gradient 倒序
</span><span class="mi">78</span>         <span class="n">dtemp</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W_vocab</span><span class="sh">'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b_vocab</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">temporal_affine_backward</span><span class="p">(</span><span class="n">dout</span><span class="p">,</span> <span class="n">temporal_cache</span><span class="p">)</span>
<span class="mi">79</span>         <span class="n">drnn</span><span class="p">,</span> <span class="n">dh0</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">Wx</span><span class="sh">'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">Wh</span><span class="sh">'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">rnn_backward</span><span class="p">(</span><span class="n">dtemp</span><span class="p">,</span> <span class="n">rnn_cache</span><span class="p">)</span>
<span class="mi">80</span>         <span class="n">dfeatures</span><span class="p">,</span> <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W_proj</span><span class="sh">'</span><span class="p">],</span> <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">b_proj</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">affine_backward</span><span class="p">(</span><span class="n">dh0</span><span class="p">,</span> <span class="n">feature_cache</span><span class="p">)</span>
<span class="mi">81</span>      
<span class="mi">82</span>         <span class="n">grads</span><span class="p">[</span><span class="sh">'</span><span class="s">W_embed</span><span class="sh">'</span><span class="p">]</span> <span class="o">=</span> <span class="nf">word_embedding_backward</span><span class="p">(</span><span class="n">drnn</span><span class="p">,</span> <span class="n">emb_cache</span><span class="p">)</span>
<span class="mi">83</span>         <span class="c1">#pass
</span><span class="mi">84</span>         <span class="c1">############################################################################
</span><span class="mi">85</span>         <span class="c1">#                             END OF YOUR CODE                             #
</span><span class="mi">86</span>         <span class="c1">############################################################################
</span><span class="mi">87</span> 
<span class="mi">88</span>         <span class="k">return</span> <span class="n">loss</span><span class="p">,</span> <span class="n">grads</span>
</code></pre></div></div> <p>This function basically implement the process shown in the image that we saw in the very begining.</p> <p>AFTER finishing this function, in the cs231n assignment3 file, they also present a function that is used for overfitting small data, only in this way can we know that this model can be used. So don’t forget to overfit small data first right after finishing your model!</p> <p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_120908.jpg" alt="2018-08-03_120908.jpg"></p> <p>If you see this image showing then you should have a big smile on your face : )</p> <p>And the description of the image will be start with the <start> tp=token and end with <end> token, the result will be like : ![2018-08-03_121318.jpg](https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_1213181.jpg)</end></start></p> <p> </p> <p><img src="https://zhengliangliang.files.wordpress.com/2018/08/2018-08-03_1213411.jpg" alt="2018-08-03_121341.jpg"></p> <p> </p> <p>TBH, it could be really hard to write the code from the very begining and build all the functions and connect them based on logical basis, but if you can imagine the whole process or how the data will be transported and calculated, after imaging so,recalling the one specific process image in your mind, then the functions can be easier to be implemented. Tips: Every time you need to do dot product, make sure you know the size of the elements!!!</p> <p>3rd Aug 2018</p> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/google-gemini-updates-flash-15-gemma-2-and-project-astra/">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/displaying-external-posts-on-your-al-folio-blog/">Displaying External Posts on Your al-folio Blog</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2024/What-is-Mathematics-CH1-Solution/">What is Mathematics: Solution Chapter 1</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2023/small-guide-to-supplements-what-you-need-to-know/">A small guide to supplements: What you need to know</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/">混乱与秩序</a> </li> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>let giscusTheme=determineComputedTheme(),giscusAttributes={src:"https://giscus.app/client.js","data-repo":"ZhengLiangliang1996/zhengliangliang1996.github.io","data-repo-id":"","data-category":"Comments","data-category-id":"","data-mapping":"title","data-strict":"1","data-reactions-enabled":"1","data-emit-metadata":"0","data-input-position":"bottom","data-theme":giscusTheme,"data-lang":"en",crossorigin:"anonymous",async:""},giscusScript=document.createElement("script");Object.entries(giscusAttributes).forEach(([t,e])=>giscusScript.setAttribute(t,e)),document.getElementById("giscus_thread").appendChild(giscusScript);</script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2024 Liangliang Zheng. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?06cae41083477f121be8cd9797ad8e2f"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.min.js" integrity="sha256-rjmgmaB99riUNcdlrDtcAiwtLIojSxNyUFdl+Qh+rB4=" crossorigin="anonymous"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>addBackToTop();</script> <script type="module" src="/assets/js/search/ninja-keys.min.js?601a2d3465e2a52bec38b600518d5f70"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script>let searchTheme=determineComputedTheme();const ninjaKeys=document.querySelector("ninja-keys");"dark"===searchTheme?ninjaKeys.classList.add("dark"):ninjaKeys.classList.remove("dark");const openSearchModal=()=>{const e=$("#navbarNav");e.hasClass("show")&&e.collapse("hide"),ninjaKeys.open()};</script> <script>const ninja=document.querySelector("ninja-keys");ninja.data=[{id:"nav-about",title:"about",section:"Navigation",handler:()=>{window.location.href="/"}},{id:"nav-blog",title:"blog",description:"",section:"Navigation",handler:()=>{window.location.href="/blog/"}},{id:"nav-repositories",title:"repositories",description:"",section:"Navigation",handler:()=>{window.location.href="/repositories/"}},{id:"nav-cv",title:"CV",description:"Personal CV, updated on 15 Jun, 2024",section:"Navigation",handler:()=>{window.location.href="/cv/"}},{id:"dropdown-by-year",title:"by year",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"dropdown-by-category",title:"by category",description:"",section:"Dropdown",handler:()=>{window.location.href=""}},{id:"post-google-gemini-updates-flash-1-5-gemma-2-and-project-astra",title:"Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra",description:"We\u2019re sharing updates across our Gemini family of models and a glimpse of Project Astra, our vision for the future of AI assistants.",section:"Posts",handler:()=>{window.open("https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/","_blank")}},{id:"post-what-is-mathematics-solution-chapter-1",title:"What is Mathematics: Solution Chapter 1",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2024/What-is-Mathematics-CH1-Solution/"}},{id:"post-a-small-guide-to-supplements-what-you-need-to-know",title:"A small guide to supplements: What you need to know",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2023/small-guide-to-supplements-what-you-need-to-know/"}},{id:"post-\u6df7\u4e71\u4e0e\u79e9\u5e8f",title:"\u6df7\u4e71\u4e0e\u79e9\u5e8f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/%E6%B7%B7%E4%B9%B1%E4%B8%8E%E7%A7%A9%E5%BA%8F/"}},{id:"post-podcast-notes-huberman-lab-dopomine",title:"Podcast Notes: Huberman Lab Dopomine",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/podcast-notes-huberman-lab-dopomine/"}},{id:"post-displaying-external-posts-on-your-al-folio-blog",title:"Displaying External Posts on Your al-folio Blog",description:"",section:"Posts",handler:()=>{window.open("https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2","_blank")}},{id:"post-erc-721",title:"ERC 721",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/erc-721/"}},{id:"post-cost-functions-and-its-properties-in-deep-learning-tbc",title:"Cost Functions and its properties in Deep Learning (TBC)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/cost-functions-and-its-properties-in-deep-learning-tbc/"}},{id:"post-cv-modifying",title:"CV: Modifying",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2022/cv-modifying/"}},{id:"post-cheatsheet-pandas-dataframe-commonly-used",title:"Cheatsheet: Pandas, Dataframe, (commonly used)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/cheatsheet-pandas-dataframe-seriescommonly-used/"}},{id:"post-\u4eba\u751f\u4f55\u5904\u4e0d\u6ee5\u60c5",title:"\u4eba\u751f\u4f55\u5904\u4e0d\u6ee5\u60c5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/%E4%BA%BA%E7%94%9F%E4%BD%95%E5%A4%84%E4%B8%8D%E6%BB%A5%E6%83%85/"}},{id:"post-hash-bang-bin-bash-tbc",title:"hash bang/bin/bash (tbc)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/hash-bang-bin-bash-tbc/"}},{id:"post-mit-15-s12-blockchain-and-money-note-lec-7-9-tbc",title:"MIT 15.S12 Blockchain and Money Note (Lec 7~9) tbc",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/mit-15-s12-blockchain-and-money-note-lec-79-tbc/"}},{id:"post-mit-15-s12-blockchain-and-money-note-lec-6",title:"MIT 15.S12 Blockchain and Money Note (Lec 6)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/mit-15-s12-blockchain-and-money-note-lec-6-tbc/"}},{id:"post-mit-15-s12-blockchain-and-money-note-lec-5",title:"MIT 15.S12 Blockchain and Money Note (Lec 5)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/mit-15-s12-blockchain-and-money-note-lec-5/"}},{id:"post-awk-sed-grep-\u603b\u7ed3",title:"awk, sed, grep \u603b\u7ed3",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/awk-sed-grep-%E6%80%BB%E7%BB%93/"}},{id:"post-mit-15-s12-blockchain-and-money-note-lec-4",title:"MIT 15.S12 Blockchain and Money Note (Lec 4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/mit-15-s12-blockchain-and-money-note-lec-4/"}},{id:"post-mit-15-s12-blockchain-and-money-note-lec-3",title:"MIT 15.S12 Blockchain and Money Note (Lec 3)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/mit-15-s12-blockchain-and-money-note-lec-3/"}},{id:"post-mit-15-s12-blockchain-and-money-note-lec-2",title:"MIT 15.S12 Blockchain and Money Note (Lec 2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/mit-15-s12-blockchain-and-money-note-lec-2/"}},{id:"post-mit-15-s12-blockchain-and-money-note-lec-1",title:"MIT 15.S12 Blockchain and Money Note (Lec 1)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/15-s12-blockchain-and-money-note1/"}},{id:"post-\u6d45\u5c1d-39-\u5219-39-\u6b62-\u6b63\u5219",title:"\u6d45\u5c1d'\u5219'\u6b62: \u6b63\u5219",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/%E6%B5%85%E5%B0%9D%E5%88%99%E6%AD%A2-%E6%AD%A3%E5%88%99%E4%B8%8E%E5%85%B6%E5%BC%95%E6%93%8E%E5%AE%9E%E7%8E%B0tbc/"}},{id:"post-python\u5de9\u56fa\u6574\u7406",title:"Python\u5de9\u56fa\u6574\u7406",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2021/python%E8%BF%9B%E9%98%B6%E5%B7%A9%E5%9B%BA%E6%95%B4%E7%90%86/"}},{id:"post-\u522b\u867d\u7136\u4f46\u662f\u4e86-2020\u8fc7\u53bb\u4e86",title:"\u522b\u867d\u7136\u4f46\u662f\u4e86\uff0c2020\u8fc7\u53bb\u4e86\uff0e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E5%88%AB%E8%99%BD%E7%84%B6%E4%BD%86%E6%98%AF%E4%BA%86-2020%E8%BF%87%E5%8E%BB%E4%BA%86/"}},{id:"post-sql\u89c4\u8303\u4e0e\u6280\u5de7-\u6301\u7eed\u66f4\u65b0",title:"SQL\u89c4\u8303\u4e0e\u6280\u5de7(\u6301\u7eed\u66f4\u65b0)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/sql%E8%A7%84%E8%8C%83%E4%B8%8E%E6%8A%80%E5%B7%A7/"}},{id:"post-\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0-3",title:"\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0(3)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/"}},{id:"post-\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0-2",title:"\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0(2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/"}},{id:"post-\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0-1",title:"\u91d1\u878d\u8bfb\u4e66\u7b14\u8bb0(1)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E9%87%91%E8%9E%8D%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B01/"}},{id:"post-gpt-3\u662f\u4ec0\u4e48",title:"GPT-3\u662f\u4ec0\u4e48?",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/gpt-3/"}},{id:"post-\u524d\u8111\u767d\u8d28\u5207\u9664\u672f",title:"\u524d\u8111\u767d\u8d28\u5207\u9664\u672f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/%E5%89%8D%E8%84%91%E7%99%BD%E8%B4%A8%E5%88%87%E9%99%A4%E6%9C%AF/"}},{id:"post-personal-github",title:"Personal Github",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2020/person-github/"}},{id:"post-\u6c38\u522b\u4e86\u54402019",title:"\u6c38\u522b\u4e86\u54402019",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/%E6%B0%B8%E5%88%AB%E4%BA%86%E5%91%802019/"}},{id:"post-emacs-\u5165\u95e8-\u5360\u5751",title:"Emacs \u5165\u95e8(\u5360\u5751)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/emacs-%E5%85%A5%E9%97%A8%E5%8D%A0%E5%9D%91/"}},{id:"post-\u6211\u5728\u5907\u5fd8\u5f55\u90fd\u5199\u4e9b\u4ec0\u4e48",title:"\u6211\u5728\u5907\u5fd8\u5f55\u90fd\u5199\u4e9b\u4ec0\u4e48\uff1f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/%E6%8C%96%E5%9D%91%E8%BF%87%E5%8E%BB%E7%9A%84%E4%B8%80%E5%B9%B4/"}},{id:"post-adam-vs-radam",title:"Adam vs. RAdam",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2019/%E5%8D%A0%E5%9D%91-adam-vs-radam/"}},{id:"post-\u7855\u58eb\u5f00\u5b66\u7b2c\u4e00\u5b66\u671f-\u5173\u4e8e\u7410\u4e8b\u548c\u5b66\u4e60\u53ca\u65b9\u6cd5",title:"\u7855\u58eb\u5f00\u5b66\u7b2c\u4e00\u5b66\u671f\u2014\u2014\u5173\u4e8e\u7410\u4e8b\u548c\u5b66\u4e60\u53ca\u65b9\u6cd5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/%E7%A1%95%E5%A3%AB%E5%BC%80%E5%AD%A6%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F-%E5%85%B3%E4%BA%8E%E7%90%90%E4%BA%8B%E5%92%8C%E5%AD%A6%E4%B9%A0%E5%8F%8A%E6%96%B9%E6%B3%95/"}},{id:"post-emotion-recognition-based-on-tensorflow",title:"Emotion Recognition Based on Tensorflow",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/emotion-recognition-based-on-tensorflow/"}},{id:"post-python-review-amp-amp-some-simple-algorithms",title:"Python_review &amp; Some simple algorithms",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/python_review-some-simple-algorithms/"}},{id:"post-nlp-word2vec-skip-gram-cs224n-implemented-in-raw-way-and-in-tensorflow",title:"NLP: Word2Vec Skip-Gram(CS224n) implemented in raw way and in Tensorflow",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/nlp-word2vec-skip-gramcs224n/"}},{id:"post-cs231n-assignment3-image-captioning-with-rnns",title:"CS231n Assignment3_Image Captioning with RNNs",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/cs231n-assignment3_image-captioning-with-rnns/"}},{id:"post-pytorch-vs-tensorflow",title:"Pytorch VS Tensorflow",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/pytorch-vs-tensorflow/"}},{id:"post-cs231n-cnn-notes-amp-amp-assignment2",title:"CS231N_CNN_Notes&amp;Assignment2",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/cs231n_cnn_notesassignment2/"}},{id:"post-web-crawler\u722ctop100\u7535\u5f71\u4fe1\u606f",title:"Web_Crawler\u722ctop100\u7535\u5f71\u4fe1\u606f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2018/web_crawler%E7%88%ACtop100%E7%94%B5%E5%BD%B1%E4%BF%A1%E6%81%AF/"}},{id:"post-\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3day2t1234",title:"\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3Day2T1234",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day2t1234/"}},{id:"post-\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3day1t234",title:"\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3Day1T234",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93day1t234%E9%9B%B6%E5%92%8C%E5%8D%9A%E5%BC%88/"}},{id:"post-\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3day1t1-\u96f6\u548c\u535a\u5f08",title:"\u725b\u5ba2\u7b97\u6cd5\u76f4\u64ad\u9898\u76ee\u603b\u7ed3Day1T1(\u96f6\u548c\u535a\u5f08)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/%E7%89%9B%E5%AE%A2%E7%AE%97%E6%B3%95%E7%9B%B4%E6%92%AD%E9%A2%98%E7%9B%AE%E6%80%BB%E7%BB%93/"}},{id:"post-java\u5b66\u4e60\u7b14\u8bb0-\u516d-\u88c5\u9970-\u89c2\u5bdf\u8005\u6a21\u5f0f\u53ca\u76d1\u542c\u5668",title:"Java\u5b66\u4e60\u7b14\u8bb0(\u516d):\u88c5\u9970\u3001\u89c2\u5bdf\u8005\u6a21\u5f0f\u53ca\u76d1\u542c\u5668",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%85%AD%E8%A3%85%E9%A5%B0-%E8%A7%82%E5%AF%9F%E8%80%85%E6%A8%A1%E5%BC%8F%E5%8F%8A%E7%9B%91%E5%90%AC%E5%99%A8/"}},{id:"post-java\u5b66\u4e60\u7b14\u8bb0-\u56db-filter",title:"Java\u5b66\u4e60\u7b14\u8bb0(\u56db):Filter",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E5%9B%9Bfilter/"}},{id:"post-java\u5b66\u4e60\u7b14\u8bb0-\u4e94-\u6587\u4ef6\u7684\u4e0a\u4f20\u548c\u4e0b\u8f7d",title:"Java\u5b66\u4e60\u7b14\u8bb0(\u4e94):\u6587\u4ef6\u7684\u4e0a\u4f20\u548c\u4e0b\u8f7d",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/java%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%BA%94%E6%96%87%E4%BB%B6%E7%9A%84%E4%B8%8A%E4%BC%A0%E5%92%8C%E4%B8%8B%E8%BD%BD/"}},{id:"post-jdbc\u5b66\u4e60\u7b14\u8bb0-\u4e09",title:"JDBC\u5b66\u4e60\u7b14\u8bb0(\u4e09)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/jdbc%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0%E4%B8%89/"}},{id:"post-jdbc\u5b66\u4e60\u7b14\u8bb0-\u4e8c",title:"JDBC\u5b66\u4e60\u7b14\u8bb0(\u4e8c)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E4%BA%8C/"}},{id:"post-jdbc\u5b66\u4e60\u7b14\u8bb0-\u4e00",title:"JDBC\u5b66\u4e60\u7b14\u8bb0(\u4e00)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/jdbc%E7%AC%94%E8%AE%B0%E5%8F%8A%E5%85%B6%E4%BB%96/"}},{id:"post-j2ee-cookie\u4e0esession\u77e5\u8bc6\u70b9\u56de\u987e",title:"J2EE_Cookie\u4e0eSession\u77e5\u8bc6\u70b9\u56de\u987e",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/j2ee_cookie%E4%B8%8Esession%E7%9F%A5%E8%AF%86%E7%82%B9%E5%9B%9E%E9%A1%BE/"}},{id:"post-bug-\u89e3\u51b3-nginx-502-bad-gateway",title:"Bug\uff1a\u89e3\u51b3 nginx 502 Bad GateWay",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2017/bug-%E8%A7%A3%E5%86%B3-nginx-502-bad-gateway/"}},{id:"post-windows-laravel\u73af\u5883\u90e8\u7f72",title:"Windows Laravel\u73af\u5883\u90e8\u7f72",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/windows-laravel%E7%8E%AF%E5%A2%83%E9%83%A8%E7%BD%B2/"}},{id:"post-git\u57fa\u7840\u8bb0\u5f55",title:"Git\u57fa\u7840\u8bb0\u5f55",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/git%E5%9F%BA%E7%A1%80%E8%AE%B0%E5%BD%95/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-6",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(6)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B06/"}},{id:"post-qt\u5b9e\u73b0\u7535\u5b50\u8bcd\u5178gui-project",title:"QT\u5b9e\u73b0\u7535\u5b50\u8bcd\u5178GUI\uff08project\uff09",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/qt%E5%AE%9E%E7%8E%B0%E7%94%B5%E5%AD%90%E8%AF%8D%E5%85%B8gui-project/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-5",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(5)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B05/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-4",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(4)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B04/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-3",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(3)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B03/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-2",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0(2)",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B02/"}},{id:"post-aws\u514d\u8d39\u670d\u52a1\u5668\u7533\u8bf7\u548c\u4e2d\u7ee7\u5668\u642d\u5efa",title:"AWS\u514d\u8d39\u670d\u52a1\u5668\u7533\u8bf7\u548c\u4e2d\u7ee7\u5668\u642d\u5efa",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/aws%E5%85%8D%E8%B4%B9%E6%9C%8D%E5%8A%A1%E5%99%A8%E7%94%B3%E8%AF%B7%E5%92%8C%E4%B8%AD%E7%BB%A7%E5%99%A8%E6%90%AD%E5%BB%BA/"}},{id:"post-\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0-1",title:"\u7b97\u6cd5\u5bfc\u8bba\u8bfb\u4e66\u7b14\u8bb0\uff081\uff09",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2016/%E7%AE%97%E6%B3%95%E5%AF%BC%E8%AE%BA%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0-1/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u6392\u5e8f",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u6392\u5e8f",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%8E%92%E5%BA%8F/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u56fe",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u56fe",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E5%9B%BE/"}},{id:"post-stl\u4e4bvector\u7528\u6cd5",title:"STL\u4e4bVector\u7528\u6cd5",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/stl%E4%B9%8Bvector%E7%94%A8%E6%B3%95/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u6811",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u6811",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%91/"}},{id:"post-\u56de\u6eaf\u6cd5\u7ecf\u5178\u5e94\u7528-n\u7687\u540e-amp-amp-\u8ff7\u5bab\u95ee\u9898",title:"\u56de\u6eaf\u6cd5\u7ecf\u5178\u5e94\u7528\u2014\u2014N\u7687\u540e&amp;\u8ff7\u5bab\u95ee\u9898",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E5%9B%9E%E6%BA%AF%E6%B3%95%E7%BB%8F%E5%85%B8%E5%BA%94%E7%94%A8-n%E7%9A%87%E5%90%8E%E8%BF%B7%E5%AE%AB%E9%97%AE%E9%A2%98/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u961f\u5217",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u961f\u5217",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%98%9F%E5%88%97/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u6808",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u6808",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E6%A0%88/"}},{id:"post-\u5927\u4e8c\u7b2c\u4e00\u5b66\u671f\u8ba1\u5212-\u5907\u8868\u53ca\u5176\u4ed6",title:"\u5927\u4e8c\u7b2c\u4e00\u5b66\u671f\u8ba1\u5212/\u5907\u8868\u53ca\u5176\u4ed6",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E5%A4%A7%E4%BA%8C%E7%AC%AC%E4%B8%80%E5%AD%A6%E6%9C%9F%E8%AE%A1%E5%88%92%E5%A4%87%E8%A1%A8%E5%8F%8A%E5%85%B6%E4%BB%96/"}},{id:"post-\u5229\u7528\u9753\u6c64beautifulsoup4\u5199\u4e00\u4e2a\u7b80\u6613\u722c\u866b",title:"\u5229\u7528\u9753\u6c64BeautifulSoup4\u5199\u4e00\u4e2a\u7b80\u6613\u722c\u866b",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E5%88%A9%E7%94%A8%E9%9D%93%E6%B1%A4beautifulsoup4%E5%86%99%E4%B8%80%E4%B8%AA%E7%AE%80%E6%98%93%E7%88%AC%E8%99%AB/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u94fe\u8868",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u94fe\u8868",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B9%8B-%E9%93%BE%E8%A1%A8/"}},{id:"post-\u6570\u636e\u7ed3\u6784\u4e4b-\u5411\u91cf",title:"\u6570\u636e\u7ed3\u6784\u4e4b\u2014\u2014\u5411\u91cf",description:"",section:"Posts",handler:()=>{window.location.href="/blog/2015/vector/"}},{id:"news-a-simple-inline-announcement-with-markdown-emoji-sparkles-smile",title:'A simple inline announcement with Markdown emoji! <img class="emoji" title=":sparkles:" alt=":sparkles:" src="https://github.githubassets.com/images/icons/emoji/unicode/2728.png" height="20" width="20"> <img class="emoji" title=":smile:" alt=":smile:" src="https://github.githubassets.com/images/icons/emoji/unicode/1f604.png" height="20" width="20">',description:"",section:"News"},{id:"news-a-long-announcement-with-details",title:"A long announcement with details",description:"",section:"News",handler:()=>{window.location.href="/news/announcement_2/"}},{id:"news-a-simple-inline-announcement",title:"A simple inline announcement.",description:"",section:"News"},{id:"projects-project-1",title:"project 1",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/1_project/"}},{id:"projects-project-2",title:"project 2",description:"a project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/2_project/"}},{id:"projects-project-3-with-very-long-name",title:"project 3 with very long name",description:"a project that redirects to another website",section:"Projects",handler:()=>{window.location.href="/projects/3_project/"}},{id:"projects-project-4",title:"project 4",description:"another without an image",section:"Projects",handler:()=>{window.location.href="/projects/4_project/"}},{id:"projects-project-5",title:"project 5",description:"a project with a background image",section:"Projects",handler:()=>{window.location.href="/projects/5_project/"}},{id:"projects-project-6",title:"project 6",description:"a project with no image",section:"Projects",handler:()=>{window.location.href="/projects/6_project/"}},{id:"projects-project-7",title:"project 7",description:"with background image",section:"Projects",handler:()=>{window.location.href="/projects/7_project/"}},{id:"projects-project-8",title:"project 8",description:"an other project with a background image and giscus comments",section:"Projects",handler:()=>{window.location.href="/projects/8_project/"}},{id:"projects-project-9",title:"project 9",description:"another project with an image \ud83c\udf89",section:"Projects",handler:()=>{window.location.href="/projects/9_project/"}},{id:"socials-email",title:"Send email",section:"Socials",handler:()=>{window.open("mailto:%7A%68%65%6E%67%6C%69%61%6E%67%6C%69%61%6E%67%31%39%39%37@%67%6D%61%69%6C.%63%6F%6D","_blank")}},{id:"socials-rss",title:"RSS Feed",section:"Socials",handler:()=>{window.open("/feed.xml","_blank")}},{id:"light-theme",title:"Change theme to light",description:"Change the theme of the site to Light",section:"Theme",handler:()=>{setThemeSetting("light")}},{id:"dark-theme",title:"Change theme to dark",description:"Change the theme of the site to Dark",section:"Theme",handler:()=>{setThemeSetting("dark")}},{id:"system-theme",title:"Use system default theme",description:"Change the theme of the site to System Default",section:"Theme",handler:()=>{setThemeSetting("system")}}];</script> <script src="/assets/js/shortcut-key.js"></script> </body> </html>